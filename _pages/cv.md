---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Education
======
* B.Tech in Computer Science, Maharshi Dayanand University, Present  


Work Experience
======
### AICTE & Edunet Foundation (in collaboration with IBM Skillsbuild)
**Machine Learning Summer Intern**  
*Jun 2023 – Aug 2023*  
- Developed an AI Mental Fitness Tracker integrating diverse machine learning algorithms: Linear Regression, Decision Tree, XGBoost, KNN, SVM, and Random Forest.
- Employed MSE, RMSE, and R-squared metrics for algorithmic evaluation and optimization in the project.
- Achieved an exceptional 99.37% accuracy on testing data with the Random Forest Model, demonstrating strong predictive modeling skills.

### ACM
**AI Intern - Bigrams to Transformers**  
*Jun 2024 – Jul 2024*  
- Developed Character-Level Language models, progressing from a basic Bigram model to a Multi-Layer Perceptron (MLP) architecture.
- Explored MLP internals, analyzing forward pass activations and backward pass gradients to understand scaling challenges.
- Implemented Batch Normalization for improved training of deep neural networks, addressing issues such as vanishing or exploding gradients.
- Built a Generatively Pretrained Transformer (GPT) model based on "Attention is All You Need" and OpenAI's GPT-2 / GPT-3, creating a robust language generation system.

### Cohere For AI
**Community Research Associate – Multilingual LLM Evaluation**  
*Jan 2025 – Present*  
- Conducting in-depth research on multilingual benchmarks through continuous literature review, identifying gaps in long-context generation and reasoning across diverse languages.
- Collaborating on the ongoing design and implementation of a data creation pipeline that curates diverse multilingual datasets to rigorously evaluate and enhance LLM performance in complex reasoning tasks.

**Community Research Associate – Multilingual Long-Context NLP**  
*Jan 2025 – Present*  
- Exploring advanced techniques to address long-context challenges in multilingual NLP by evaluating position encoding methods (RoPE, NoPE, LongRoPE) alongside model architectures (SSMs and Hybrid Transformer-SSM) to improve scalability and efficiency.
- Developing a novel approach that integrates SSMs with Transformers to enhance long-context multilingual understanding while reducing computational overhead.



Skills
======
### Programming Languages
- Python
- C
- C++
- SQL
- PostgreSQL
- Data Structures

### Data Analytics Skills
- Power BI
- Tableau
- Excel
- Pandas

### AI Technologies
- Machine Learning
- Deep Learning
- Natural Language Processing (NLP)
- Computer Vision
- Fine-Tuning
- Peft
- RLHF
- RAG
- DPO
- Docker
- Git

### Programming Frameworks
- PyTorch
- TensorFlow
- Scikit-learn
- Huggingface
- Langchain
- Numpy
- Pandas
- Matplotlib
- Django
- Django Rest Framework

Publications
======
  <ul>{% for post in site.publications reversed %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul>
  
Talks
======
  <ul>{% for post in site.talks reversed %}
    {% include archive-single-talk-cv.html  %}
  {% endfor %}</ul>
  
Teaching
======
  <ul>{% for post in site.teaching reversed %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul>
  
Service and leadership
======
* Currently signed in to 43 different slack teams
